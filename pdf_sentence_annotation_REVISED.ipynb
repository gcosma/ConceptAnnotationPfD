{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Sentence Extraction and Multi-Label Theme Classification\n",
    "## Fine-tuned Model for Prevention of Future Deaths Report Analysis\n",
    "\n",
    "**Version 2.0 - Fixed for small datasets with class imbalance**\n",
    "\n",
    "This notebook:\n",
    "1. Extracts sentences from PDF documents\n",
    "2. Fine-tunes a transformer model for multi-label classification\n",
    "3. Handles encoding issues and class imbalance\n",
    "4. Includes data augmentation for rare classes\n",
    "5. Annotates sentences with theme codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets torch pdfplumber nltk scikit-learn pandas numpy accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdfplumber\n",
    "import nltk\n",
    "import torch\n",
    "import re\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Download NLTK data for sentence tokenization\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Upload Your Files\n",
    "\n",
    "Upload:\n",
    "1. Your ground truth CSV file (`ground_truth_Annotations_.csv`)\n",
    "2. PDF files you want to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"Upload your ground truth CSV file:\")\n",
    "uploaded = files.upload()\n",
    "csv_filename = list(uploaded.keys())[0]\n",
    "print(f\"Uploaded: {csv_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Explore Ground Truth Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file with encoding handling\n",
    "def load_csv_with_encoding(filename):\n",
    "    \"\"\"Try multiple encodings to load CSV file\"\"\"\n",
    "    encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(filename, encoding=encoding)\n",
    "            print(f\"Successfully loaded with {encoding} encoding\")\n",
    "            return df\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    \n",
    "    # If all fail, use latin-1 with error handling\n",
    "    print(\"Using latin-1 encoding with error handling\")\n",
    "    return pd.read_csv(filename, encoding='latin-1', encoding_errors='replace')\n",
    "\n",
    "df = load_csv_with_encoding(csv_filename)\n",
    "\n",
    "print(\"\\nDataset shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Data for Multi-Label Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_theme_codes(theme_str):\n",
    "    \"\"\"Parse theme codes from string format (e.g., 'C1, O4' or 'H1')\"\"\"\n",
    "    if pd.isna(theme_str):\n",
    "        return []\n",
    "    # Remove quotes and split by comma\n",
    "    themes = [t.strip().strip('\"') for t in str(theme_str).split(',')]\n",
    "    return [t for t in themes if t]  # Remove empty strings\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by replacing encoding artifacts\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return text\n",
    "    replacements = {\n",
    "        '\\x91': \"'\",  # Left single quote\n",
    "        '\\x92': \"'\",  # Right single quote  \n",
    "        '\\x93': '\"',  # Left double quote\n",
    "        '\\x94': '\"',  # Right double quote\n",
    "        '\\x96': '-',  # En dash\n",
    "        '\\x97': '-',  # Em dash\n",
    "    }\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    return text\n",
    "\n",
    "# Clean sentences in the dataframe\n",
    "df['sentence'] = df['sentence'].apply(clean_text)\n",
    "\n",
    "# Parse theme codes\n",
    "df['theme_list'] = df['themes'].apply(parse_theme_codes)\n",
    "\n",
    "# Get all unique theme codes\n",
    "all_themes = set()\n",
    "for themes in df['theme_list']:\n",
    "    all_themes.update(themes)\n",
    "all_themes = sorted(list(all_themes))\n",
    "\n",
    "print(f\"Total unique theme codes: {len(all_themes)}\")\n",
    "print(f\"\\nTheme codes: {all_themes}\")\n",
    "\n",
    "# Count theme frequencies\n",
    "theme_counts = Counter([theme for themes in df['theme_list'] for theme in themes])\n",
    "print(\"\\nTheme distribution:\")\n",
    "for theme, count in theme_counts.most_common():\n",
    "    print(f\"{theme}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-label binary encoding\n",
    "mlb = MultiLabelBinarizer(classes=all_themes)\n",
    "labels_encoded = mlb.fit_transform(df['theme_list'])\n",
    "\n",
    "print(f\"Label matrix shape: {labels_encoded.shape}\")\n",
    "print(f\"Number of labels per sample (min/mean/max): {labels_encoded.sum(axis=1).min():.2f} / {labels_encoded.sum(axis=1).mean():.2f} / {labels_encoded.sum(axis=1).max():.2f}\")\n",
    "\n",
    "# Create mapping dictionaries\n",
    "id2label = {idx: label for idx, label in enumerate(all_themes)}\n",
    "label2id = {label: idx for idx, label in enumerate(all_themes)}\n",
    "\n",
    "print(f\"\\nLabel mappings created for {len(all_themes)} themes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "sentences = df['sentence'].tolist()\n",
    "\n",
    "# Split data (80% train, 10% validation, 10% test)\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "    sentences, labels_encoded, test_size=0.2, random_state=42, stratify=None\n",
    ")\n",
    "\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    temp_texts, temp_labels, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_texts)}\")\n",
    "print(f\"Validation samples: {len(val_texts)}\")\n",
    "print(f\"Test samples: {len(test_texts)}\")\n",
    "\n",
    "# Convert labels to float32 for multi-label classification\n",
    "train_labels = train_labels.astype(np.float32)\n",
    "val_labels = val_labels.astype(np.float32)\n",
    "test_labels = test_labels.astype(np.float32)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = Dataset.from_dict({'text': train_texts, 'labels': train_labels.tolist()})\n",
    "val_dataset = Dataset.from_dict({'text': val_texts, 'labels': val_labels.tolist()})\n",
    "test_dataset = Dataset.from_dict({'text': test_texts, 'labels': test_labels.tolist()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Augmentation for Minority Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation through paraphrasing and synonym replacement\n",
    "def augment_sentence(sentence):\n",
    "    \"\"\"Simple augmentation by synonym replacement\"\"\"\n",
    "    # Common replacements in healthcare context\n",
    "    replacements = {\n",
    "        'did not': ['failed to', 'neglected to', 'omitted to', 'did not'],\n",
    "        'lack of': ['absence of', 'insufficient', 'inadequate', 'lack of'],\n",
    "        'delay': ['postponement', 'wait', 'holdup', 'delay'],\n",
    "        'no ': ['absence of ', 'lacking ', 'without ', 'no '],\n",
    "        'staff': ['personnel', 'workers', 'employees', 'staff'],\n",
    "        'training': ['education', 'instruction', 'preparation', 'training'],\n",
    "        'communication': ['information sharing', 'dialogue', 'correspondence', 'communication'],\n",
    "        'assessment': ['evaluation', 'review', 'examination', 'assessment'],\n",
    "    }\n",
    "    \n",
    "    sentence_lower = sentence.lower()\n",
    "    for orig, syns in replacements.items():\n",
    "        if orig in sentence_lower:\n",
    "            # Pick a random synonym (might be the original)\n",
    "            chosen = random.choice(syns)\n",
    "            if chosen != orig:  # Only replace if different\n",
    "                augmented = sentence_lower.replace(orig, chosen, 1)\n",
    "                # Capitalize first letter\n",
    "                return augmented[0].upper() + augmented[1:]\n",
    "    return sentence\n",
    "\n",
    "# Augment training data for minority classes\n",
    "augmented_texts = []\n",
    "augmented_labels = []\n",
    "\n",
    "for i, text in enumerate(train_texts):\n",
    "    augmented_texts.append(text)\n",
    "    augmented_labels.append(train_labels[i])\n",
    "    \n",
    "    # Check which themes this sample has\n",
    "    themes = [j for j in range(len(all_themes)) if train_labels[i][j] == 1]\n",
    "    theme_names = [all_themes[j] for j in themes]\n",
    "    \n",
    "    # Get counts for these themes\n",
    "    min_count = min([theme_counts[name] for name in theme_names], default=100)\n",
    "    \n",
    "    # Augment samples with rare themes (appearing < 8 times)\n",
    "    if min_count < 8:\n",
    "        # Add 1-3 augmented versions depending on rarity\n",
    "        num_augments = max(1, min(3, 8 - min_count))\n",
    "        for _ in range(num_augments):\n",
    "            aug_text = augment_sentence(text)\n",
    "            if aug_text != text:  # Only add if actually different\n",
    "                augmented_texts.append(aug_text)\n",
    "                augmented_labels.append(train_labels[i])\n",
    "\n",
    "print(f\"Original training size: {len(train_texts)}\")\n",
    "print(f\"Augmented training size: {len(augmented_texts)}\")\n",
    "print(f\"Added {len(augmented_texts) - len(train_texts)} augmented samples\")\n",
    "\n",
    "# Update training data\n",
    "train_texts = augmented_texts\n",
    "train_labels = np.array(augmented_labels, dtype=np.float32)\n",
    "\n",
    "# Recreate training dataset\n",
    "train_dataset = Dataset.from_dict({'text': train_texts, 'labels': train_labels.tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install translation library for back-translation augmentation\n",
    "!pip install -q deep-translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== ENHANCED BACK-TRANSLATION AUGMENTATION ====================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENHANCED SYNTHETIC DATA GENERATION - BACK-TRANSLATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from deep_translator import GoogleTranslator\n",
    "import time\n",
    "\n",
    "def back_translate_text(text, intermediate_lang='de'):\n",
    "    \"\"\"\n",
    "    Back-translate text for augmentation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Translate to intermediate language\n",
    "        translator_to = GoogleTranslator(source='en', target=intermediate_lang)\n",
    "        intermediate = translator_to.translate(text)\n",
    "        \n",
    "        # Small delay to avoid rate limits\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        # Translate back to English\n",
    "        translator_back = GoogleTranslator(source=intermediate_lang, target='en')\n",
    "        back_translated = translator_back.translate(intermediate)\n",
    "        \n",
    "        # Only return if meaningfully different and reasonable length\n",
    "        if back_translated != text and len(back_translated) > 10 and len(back_translated) < 200:\n",
    "            return back_translated\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Enhanced augmentation combining original simple augmentation + back-translation\n",
    "print(\"\\nGenerating enhanced synthetic data...\")\n",
    "\n",
    "enhanced_texts = []\n",
    "enhanced_labels = []\n",
    "\n",
    "# Start with all original training data\n",
    "for i, text in enumerate(train_texts):\n",
    "    enhanced_texts.append(text)\n",
    "    enhanced_labels.append(train_labels[i])\n",
    "\n",
    "# Count synthetic examples by method\n",
    "back_translation_count = 0\n",
    "\n",
    "# Generate back-translations for rare themes\n",
    "for i, text in enumerate(train_texts):\n",
    "    # Check which themes this sample has\n",
    "    themes = [j for j in range(len(all_themes)) if train_labels[i][j] == 1]\n",
    "    theme_names = [all_themes[j] for j in themes]\n",
    "    \n",
    "    # Get minimum count for these themes\n",
    "    theme_counts_dict = {all_themes[j]: theme_counts[all_themes[j]] for j in themes}\n",
    "    min_count = min(theme_counts_dict.values(), default=100)\n",
    "    \n",
    "    # Generate back-translations for themes with < 12 examples\n",
    "    if min_count < 12:\n",
    "        # Use multiple languages for diversity\n",
    "        languages = ['de', 'fr', 'es']  # German, French, Spanish\n",
    "        \n",
    "        # More augmentations for rarer themes\n",
    "        num_augments = max(1, min(3, 12 - min_count))\n",
    "        \n",
    "        for lang in languages[:num_augments]:\n",
    "            bt_text = back_translate_text(text, intermediate_lang=lang)\n",
    "            if bt_text:\n",
    "                enhanced_texts.append(bt_text)\n",
    "                enhanced_labels.append(train_labels[i])\n",
    "                back_translation_count += 1\n",
    "\n",
    "print(f\"\\n\u2713 Original training size: {len(train_texts)}\")\n",
    "print(f\"\u2713 Added via back-translation: {back_translation_count}\")\n",
    "print(f\"\u2713 Enhanced training size: {len(enhanced_texts)}\")\n",
    "print(f\"\u2713 Total synthetic examples: {len(enhanced_texts) - len(train_texts)}\")\n",
    "\n",
    "# Update training data\n",
    "train_texts = enhanced_texts\n",
    "train_labels = np.array(enhanced_labels, dtype=np.float32)\n",
    "\n",
    "# Recreate training dataset with enhanced data\n",
    "train_dataset = Dataset.from_dict({'text': train_texts, 'labels': train_labels.tolist()})\n",
    "print(\"\\n\u2713 Training dataset updated with enhanced synthetic data\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Initialize Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== MODEL SELECTION ====================\n",
    "# Choose your model by setting the selected_model variable\n",
    "# Recommended: PathologyBERT for pathology reports\n",
    "\n",
    "AVAILABLE_MODELS = {\n",
    "    # General models\n",
    "    'bert-base': 'bert-base-uncased',\n",
    "    'distilbert': 'distilbert-base-uncased',  # Faster, smaller\n",
    "    \n",
    "    # Pathology-specific model \u2b50\u2b50\u2b50\n",
    "    'pathology-bert': 'tsantos/PathologyBERT',  # SPECIALIZED PATHOLOGY\n",
    "    \n",
    "    # Healthcare/Clinical models\n",
    "    'bio-clinical-bert': 'emilyalsentzer/Bio_ClinicalBERT',  # Clinical notes\n",
    "    'pubmed-bert': 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext',\n",
    "    'biobert': 'dmis-lab/biobert-v1.1',  # Biomedical text\n",
    "    'clinical-bert': 'medicalai/ClinicalBERT',  # Clinical text\n",
    "    'bluebert': 'bionlp/bluebert_pubmed_mimic_uncased_L-12_H-768_A-12',\n",
    "    'gatortron': 'UFNLP/gatortron-base',  # Clinical notes (large)\n",
    "}\n",
    "\n",
    "MODEL_DESCRIPTIONS = {\n",
    "    'bert-base': 'Standard BERT - Good baseline, general English',\n",
    "    'distilbert': 'Smaller/faster BERT - 40% faster, 97% performance',\n",
    "    'pathology-bert': '\u2b50\u2b50\u2b50 SPECIALIZED PATHOLOGY - Trained on pathology reports',\n",
    "    'bio-clinical-bert': '\u2b50 Clinical notes (MIMIC-III) - For healthcare',\n",
    "    'pubmed-bert': '\u2b50 PubMed biomedical - For medical terminology',\n",
    "    'biobert': 'Biomedical text (PubMed)',\n",
    "    'clinical-bert': 'Clinical text',\n",
    "    'bluebert': 'PubMed + MIMIC clinical',\n",
    "    'gatortron': 'Large clinical model - Slower but powerful',\n",
    "}\n",
    "\n",
    "print(\"Available Models for Healthcare/Clinical/Pathology Text:\")\n",
    "print(\"=\" * 80)\n",
    "for key, desc in MODEL_DESCRIPTIONS.items():\n",
    "    print(f\"{key:20s} | {desc}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ==================== SELECT YOUR MODEL HERE ====================\n",
    "# Change this to try different models:\n",
    "selected_model = 'pathology-bert'  # \u2b50\u2b50\u2b50 SPECIALIZED PATHOLOGY BERT\n",
    "# selected_model = 'bio-clinical-bert'  # Alternative: clinical notes\n",
    "# selected_model = 'pubmed-bert'        # Alternative: medical terminology\n",
    "# selected_model = 'bert-base'          # Baseline comparison\n",
    "\n",
    "model_name = AVAILABLE_MODELS[selected_model]\n",
    "print(f\"\\n\u2713 Selected: {selected_model}\")\n",
    "print(f\"  Model: {model_name}\")\n",
    "print(f\"  Description: {MODEL_DESCRIPTIONS[selected_model]}\")\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"\\nLoading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"\u2713 Tokenizer loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize datasets\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch with float labels\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Ensure labels are float type\n",
    "def convert_labels_to_float(batch):\n",
    "    batch['labels'] = batch['labels'].float()\n",
    "    return batch\n",
    "\n",
    "train_dataset = train_dataset.map(convert_labels_to_float, batched=False)\n",
    "val_dataset = val_dataset.map(convert_labels_to_float, batched=False)\n",
    "test_dataset = test_dataset.map(convert_labels_to_float, batched=False)\n",
    "\n",
    "print(\"Datasets tokenized and formatted with float labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for multi-label classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(all_themes),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "print(f\"Model loaded with {len(all_themes)} output labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Define Training Configuration with Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute metrics for multi-label classification\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Apply sigmoid and threshold\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    y_pred = (probs > 0.5).int().numpy()\n",
    "    y_true = labels.astype(np.float32).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    # Per-label F1 scores\n",
    "    f1_per_label = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    \n",
    "    metrics = {\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "    }\n",
    "    \n",
    "    # Add per-label F1 scores\n",
    "    for idx, label in id2label.items():\n",
    "        metrics[f'f1_{label}'] = f1_per_label[idx]\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights to handle imbalance\n",
    "class_weights = []\n",
    "for i in range(len(all_themes)):\n",
    "    pos_samples = train_labels[:, i].sum()\n",
    "    neg_samples = len(train_labels) - pos_samples\n",
    "    if pos_samples > 0:\n",
    "        # Use square root to make weights less extreme\n",
    "        weight = np.sqrt(neg_samples / pos_samples)\n",
    "    else:\n",
    "        weight = 1.0\n",
    "    class_weights.append(weight)\n",
    "\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "print(f\"\\nClass weights computed:\")\n",
    "for theme, weight in zip(all_themes, class_weights.cpu().numpy()):\n",
    "    print(f\"{theme}: {weight:.2f}\")\n",
    "\n",
    "# Custom Trainer with weighted loss\n",
    "class WeightedMultiLabelTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Weighted BCE loss\n",
    "        loss_fct = torch.nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "        loss = loss_fct(logits, labels)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Training arguments - optimized for small datasets\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=25,  # More epochs for small dataset\n",
    "    per_device_train_batch_size=8,  # Smaller batch size\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,  # Lower learning rate\n",
    "    warmup_ratio=0.1,  # Warm up 10% of steps\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=5,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1_weighted',\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    push_to_hub=False,\n",
    "    report_to='none',\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# Initialize weighted trainer\n",
    "trainer = WeightedMultiLabelTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")\n",
    "\n",
    "print(\"\\nWeighted trainer configured and ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model('./best_model')\n",
    "tokenizer.save_pretrained('./best_model')\n",
    "print(\"Model saved to ./best_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BERT MODEL TEST SET RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"F1 Micro: {test_results['eval_f1_micro']:.4f}\")\n",
    "print(f\"F1 Macro: {test_results['eval_f1_macro']:.4f}\")\n",
    "print(f\"F1 Weighted: {test_results['eval_f1_weighted']:.4f}\")\n",
    "\n",
    "print(\"\\nPer-label F1 Scores:\")\n",
    "for label in all_themes:\n",
    "    score = test_results.get(f'eval_f1_{label}', 0)\n",
    "    print(f\"{label}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed predictions on test set\n",
    "predictions = trainer.predict(test_dataset)\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "probs = sigmoid(torch.Tensor(predictions.predictions))\n",
    "y_pred = (probs > 0.5).int().numpy()\n",
    "y_true = test_labels.astype(int)\n",
    "\n",
    "# Per-label classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    target_names=all_themes,\n",
    "    zero_division=0\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Baseline Comparison: TF-IDF + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline TF-IDF + Logistic Regression model for comparison\n",
    "print(\"Training baseline TF-IDF + Logistic Regression model...\\n\")\n",
    "\n",
    "# Use original non-augmented data for fair comparison\n",
    "train_texts_orig, temp_texts, train_labels_orig, temp_labels = train_test_split(\n",
    "    sentences, labels_encoded, test_size=0.2, random_state=42, stratify=None\n",
    ")\n",
    "val_texts_orig, test_texts_orig, val_labels_orig, test_labels_orig = train_test_split(\n",
    "    temp_texts, temp_labels, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Vectorize text\n",
    "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 3), min_df=1)\n",
    "X_train_tfidf = vectorizer.fit_transform(train_texts_orig)\n",
    "X_test_tfidf = vectorizer.transform(test_texts_orig)\n",
    "\n",
    "# Train one-vs-rest classifier\n",
    "baseline_model = OneVsRestClassifier(\n",
    "    LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42, C=1.0)\n",
    ")\n",
    "baseline_model.fit(X_train_tfidf, train_labels_orig)\n",
    "\n",
    "# Predict\n",
    "y_pred_baseline = baseline_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate\n",
    "f1_micro_baseline = f1_score(test_labels_orig, y_pred_baseline, average='micro', zero_division=0)\n",
    "f1_macro_baseline = f1_score(test_labels_orig, y_pred_baseline, average='macro', zero_division=0)\n",
    "f1_weighted_baseline = f1_score(test_labels_orig, y_pred_baseline, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BASELINE (TF-IDF + LOGISTIC REGRESSION) TEST SET RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"F1 Micro: {f1_micro_baseline:.4f}\")\n",
    "print(f\"F1 Macro: {f1_macro_baseline:.4f}\")\n",
    "print(f\"F1 Weighted: {f1_weighted_baseline:.4f}\")\n",
    "\n",
    "# Per-label scores\n",
    "f1_per_label_baseline = f1_score(test_labels_orig, y_pred_baseline, average=None, zero_division=0)\n",
    "print(\"\\nPer-label F1 Scores:\")\n",
    "for idx, label in enumerate(all_themes):\n",
    "    print(f\"{label}: {f1_per_label_baseline[idx]:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"BERT F1 Weighted: {test_results['eval_f1_weighted']:.4f}\")\n",
    "print(f\"Baseline F1 Weighted: {f1_weighted_baseline:.4f}\")\n",
    "improvement = ((test_results['eval_f1_weighted'] - f1_weighted_baseline) / f1_weighted_baseline * 100) if f1_weighted_baseline > 0 else 0\n",
    "print(f\"BERT improvement: {improvement:+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. PDF Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extract text from PDF file\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "def clean_sentence(sentence: str) -> str:\n",
    "    \"\"\"Clean extracted sentence\"\"\"\n",
    "    # Replace common encoding artifacts (Windows-1252 smart quotes, etc.)\n",
    "    replacements = {\n",
    "        '\\x91': \"'\",  # Left single quote\n",
    "        '\\x92': \"'\",  # Right single quote\n",
    "        '\\x93': '\"',  # Left double quote\n",
    "        '\\x94': '\"',  # Right double quote\n",
    "        '\\x96': '-',  # En dash\n",
    "        '\\x97': '-',  # Em dash\n",
    "    }\n",
    "    for old, new in replacements.items():\n",
    "        sentence = sentence.replace(old, new)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
    "    # Remove very short sentences (likely artifacts)\n",
    "    if len(sentence) < 10:\n",
    "        return \"\"\n",
    "    return sentence\n",
    "\n",
    "def extract_sentences_from_text(text: str) -> List[str]:\n",
    "    \"\"\"Extract and clean sentences from text\"\"\"\n",
    "    # Use NLTK sentence tokenizer\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    # Clean sentences\n",
    "    cleaned_sentences = [clean_sentence(s) for s in sentences]\n",
    "    # Remove empty sentences\n",
    "    return [s for s in cleaned_sentences if s]\n",
    "\n",
    "def extract_sentences_from_pdf(pdf_path: str) -> List[str]:\n",
    "    \"\"\"Extract sentences from PDF file\"\"\"\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    return extract_sentences_from_text(text)\n",
    "\n",
    "print(\"PDF extraction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_themes(texts: List[str], threshold: float = 0.3) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Predict themes for a list of texts\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings to classify\n",
    "        threshold: Probability threshold for positive prediction (default 0.3)\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing predictions for each text\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Apply sigmoid and threshold\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(outputs.logits).cpu().numpy()\n",
    "    \n",
    "    # Process results\n",
    "    results = []\n",
    "    for i, text in enumerate(texts):\n",
    "        pred_labels = []\n",
    "        pred_probs = {}\n",
    "        \n",
    "        for j, prob in enumerate(probs[i]):\n",
    "            label = id2label[j]\n",
    "            pred_probs[label] = float(prob)\n",
    "            if prob > threshold:\n",
    "                pred_labels.append(label)\n",
    "        \n",
    "        results.append({\n",
    "            'text': text,\n",
    "            'predicted_themes': pred_labels,\n",
    "            'probabilities': pred_probs,\n",
    "            'top_themes': sorted(pred_probs.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Inference function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Test Inference on Sample Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with some sample sentences\n",
    "test_sentences = [\n",
    "    \"The staff did not receive adequate training on the new procedures.\",\n",
    "    \"Communication between departments was poor and led to delays.\",\n",
    "    \"The physical environment was unsuitable for patient safety.\",\n",
    "    \"No robust risk assessment was conducted.\",\n",
    "    \"The care plan was not properly documented.\"\n",
    "]\n",
    "\n",
    "# Use lower threshold for better recall\n",
    "predictions = predict_themes(test_sentences, threshold=0.25)\n",
    "\n",
    "print(\"\\nSample Predictions:\")\n",
    "print(\"=\" * 80)\n",
    "for pred in predictions:\n",
    "    print(f\"\\nText: {pred['text']}\")\n",
    "    print(f\"Predicted Themes: {', '.join(pred['predicted_themes']) if pred['predicted_themes'] else 'None'}\")\n",
    "    print(\"Top 5 theme probabilities:\")\n",
    "    for theme, prob in pred['top_themes'][:5]:\n",
    "        print(f\"  {theme}: {prob:.4f}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Process PDF Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload PDF files\n",
    "print(\"Upload PDF files to process:\")\n",
    "uploaded_pdfs = files.upload()\n",
    "\n",
    "pdf_files = list(uploaded_pdfs.keys())\n",
    "print(f\"\\nUploaded {len(pdf_files)} PDF file(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_and_annotate(pdf_path: str, threshold: float = 0.3, batch_size: int = 32) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process PDF file: extract sentences and annotate with themes\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "        threshold: Probability threshold for predictions\n",
    "        batch_size: Number of sentences to process at once\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with sentences and their predicted themes\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing: {pdf_path}\")\n",
    "    \n",
    "    # Extract sentences\n",
    "    sentences = extract_sentences_from_pdf(pdf_path)\n",
    "    print(f\"Extracted {len(sentences)} sentences\")\n",
    "    \n",
    "    if not sentences:\n",
    "        print(\"No sentences extracted from PDF\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Process in batches\n",
    "    all_results = []\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        batch_results = predict_themes(batch, threshold=threshold)\n",
    "        all_results.extend(batch_results)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame([\n",
    "        {\n",
    "            'document': Path(pdf_path).stem,\n",
    "            'sentence': r['text'],\n",
    "            'predicted_themes': ', '.join(r['predicted_themes']),\n",
    "            'num_themes': len(r['predicted_themes']),\n",
    "            'top_theme': r['top_themes'][0][0] if r['top_themes'] else '',\n",
    "            'top_probability': r['top_themes'][0][1] if r['top_themes'] else 0.0\n",
    "        }\n",
    "        for r in all_results\n",
    "    ])\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Process all uploaded PDFs\n",
    "all_annotations = []\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    df_annotations = process_pdf_and_annotate(pdf_file, threshold=0.25)\n",
    "    if not df_annotations.empty:\n",
    "        all_annotations.append(df_annotations)\n",
    "        print(f\"Annotated {len(df_annotations)} sentences from {pdf_file}\")\n",
    "\n",
    "# Combine all results\n",
    "if all_annotations:\n",
    "    final_df = pd.concat(all_annotations, ignore_index=True)\n",
    "    print(f\"\\nTotal sentences annotated: {len(final_df)}\")\n",
    "    print(f\"\\nTheme distribution:\")\n",
    "    print(final_df['num_themes'].value_counts().sort_index())\n",
    "else:\n",
    "    print(\"No annotations generated\")\n",
    "    final_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. View and Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample results\n",
    "if not final_df.empty:\n",
    "    print(\"\\nSample Annotations:\")\n",
    "    print(final_df.head(20))\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_filename = 'annotated_sentences.csv'\n",
    "    final_df.to_csv(output_filename, index=False)\n",
    "    print(f\"\\nResults saved to {output_filename}\")\n",
    "    \n",
    "    # Download the file\n",
    "    files.download(output_filename)\n",
    "else:\n",
    "    print(\"No results to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Save and Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip file of the model\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "# Save label mappings\n",
    "with open('./best_model/label_mappings.json', 'w') as f:\n",
    "    json.dump({\n",
    "        'id2label': id2label,\n",
    "        'label2id': label2id,\n",
    "        'all_themes': all_themes,\n",
    "        'class_weights': class_weights.cpu().tolist()\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"Label mappings saved\")\n",
    "\n",
    "# Zip the model directory\n",
    "shutil.make_archive('trained_model', 'zip', './best_model')\n",
    "print(\"\\nModel packaged as trained_model.zip\")\n",
    "\n",
    "# Download\n",
    "files.download('trained_model.zip')\n",
    "print(\"Model zip file ready for download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Interactive Annotation Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive annotation function\n",
    "def annotate_text_interactive(text: str, threshold: float = 0.25):\n",
    "    \"\"\"\n",
    "    Annotate a single piece of text interactively\n",
    "    \"\"\"\n",
    "    predictions = predict_themes([text], threshold=threshold)\n",
    "    result = predictions[0]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TEXT:\")\n",
    "    print(text)\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"PREDICTED THEMES:\")\n",
    "    if result['predicted_themes']:\n",
    "        for theme in result['predicted_themes']:\n",
    "            prob = result['probabilities'][theme]\n",
    "            print(f\"  \u2022 {theme}: {prob:.4f}\")\n",
    "    else:\n",
    "        print(\"  No themes predicted above threshold\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"ALL THEME PROBABILITIES (Top 10):\")\n",
    "    for theme, prob in result['top_themes'][:10]:\n",
    "        print(f\"  {theme}: {prob:.4f}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Example usage - uncomment and modify to test\n",
    "# annotate_text_interactive(\"The patient was not properly assessed for risk.\")\n",
    "print(\"Interactive annotation function ready. Use annotate_text_interactive('your text here') to test.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}